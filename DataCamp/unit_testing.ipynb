{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit testing for data science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python unit test library:\n",
    "- pytest\n",
    "- unitest\n",
    "- nosetests\n",
    "- doctest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1:\n",
    "- create test_row_to_list.py\n",
    "- test_ indicate unit tests\n",
    "Step 2:\n",
    "- import pytest\n",
    "- import row_to_list\n",
    "Step 3:\n",
    "- def test_bla(): assert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_for_missing_area():\n",
    "    assert row_to_list(\"ksfjsjf\\n\") is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.9.7, pytest-6.2.5, py-1.11.0, pluggy-1.0.0\n",
      "rootdir: c:\\Users\\Lenovo\\Documents\\DataCamp\n",
      "collected 0 items\n",
      "\n",
      "============================ no tests ran in 0.00s ============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: file or directory not found: test_example()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "\n",
    "def example():\n",
    "    return 2\n",
    "\n",
    "def test_example():\n",
    "    assert example() == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.9.7, pytest-6.2.5, py-1.11.0, pluggy-1.0.0\n",
      "rootdir: c:\\Users\\Lenovo\\Documents\\DataCamp\n",
      "collected 1 item\n",
      "\n",
      "test_example.py .                                                        [100%]\n",
      "\n",
      "============================== 1 passed in 0.01s ==============================\n"
     ]
    }
   ],
   "source": [
    "!pytest test_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mastering assert statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "1 different from 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12444/1086560947.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"1 different from 2\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: 1 different from 2"
     ]
    }
   ],
   "source": [
    "assert 1==2, \"1 different from 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "c'est pas égal !",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12444/1513340769.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Don't use this for floats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[1;36m.1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m.1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m.1\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"c'est pas égal !\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: c'est pas égal !"
     ]
    }
   ],
   "source": [
    "# Don't use this for floats\n",
    "\n",
    "assert .1 + .1 + .1 == .3, \"c'est pas égal !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this instead\n",
    "import pytest\n",
    "\n",
    "assert .1 + .1 + .1 == pytest.approx(.3), \"approximation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works with numpy too\n",
    "import numpy as np\n",
    "\n",
    "assert np.array([.1 + .1, .1 + .1]) == pytest.approx(np.array([.2, .2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple assertions in one unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_int(num):\n",
    "    return int(num.replace(\",\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_string_with_one_comma():\n",
    "    test_argument = \"2,081\"\n",
    "    expected = 2081\n",
    "    actual = convert_to_int(test_argument)\n",
    "    message = \"convert_to_int('2,081') should return the int 2081, but it actually returned {0}\".format(actual)\n",
    "    assert isinstance(return_value, int)\n",
    "    assert actual == expected, message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is part of the context\n"
     ]
    },
    {
     "ename": "Failed",
     "evalue": "DID NOT RAISE <class 'ValueError'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailed\u001b[0m                                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3472/1722029722.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mpytest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraises\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# <--- Does nothing on entering the context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"This is part of the context\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# <--- If context raised ValueError, silence it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# <--- If the context did not raise ValueError, raise an exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\_pytest\\outcomes.py\u001b[0m in \u001b[0;36mfail\u001b[1;34m(msg, pytrace)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \"\"\"\n\u001b[0;32m    152\u001b[0m     \u001b[0m__tracebackhide__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mFailed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpytrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpytrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailed\u001b[0m: DID NOT RAISE <class 'ValueError'>"
     ]
    }
   ],
   "source": [
    "with pytest.raises(ValueError):\n",
    "    # <--- Does nothing on entering the context\n",
    "    print(\"This is part of the context\")\n",
    "    # <--- If context raised ValueError, silence it\n",
    "    # <--- If the context did not raise ValueError, raise an exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pytest.raises(ValueError):\n",
    "    raise ValueError  # context exists w/ ValueError\n",
    "    # <--- pytest.raises(ValueError) silences it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Failed",
     "evalue": "DID NOT RAISE <class 'ValueError'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailed\u001b[0m                                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3472/348383568.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mpytest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraises\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mpass\u001b[0m   \u001b[1;31m# context exists without raising a ValueError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# <--- pytest.raises(ValueError) raised Failed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\_pytest\\outcomes.py\u001b[0m in \u001b[0;36mfail\u001b[1;34m(msg, pytrace)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \"\"\"\n\u001b[0;32m    152\u001b[0m     \u001b[0m__tracebackhide__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mFailed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpytrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpytrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailed\u001b[0m: DID NOT RAISE <class 'ValueError'>"
     ]
    }
   ],
   "source": [
    "with pytest.raises(ValueError):\n",
    "    pass   # context exists without raising a ValueError\n",
    "# <--- pytest.raises(ValueError) raised Failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_on_numpy():\n",
    "    ex = (1.4, 2)\n",
    "    with pytest.raises(TypeError) as error:\n",
    "        np.zeros(ex)\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ExceptionInfo TypeError(\"'float' object cannot be interpreted as an integer\") tblen=1>\n"
     ]
    }
   ],
   "source": [
    "test_on_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_numpy2():\n",
    "    ex = (1.4, 2)\n",
    "    with pytest.raises(TypeError) as error:\n",
    "        np.zeros(ex)\n",
    "    expected_error_msg = \"hehe boi\"\n",
    "    assert error.match(expected_error_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Regex pattern 'hehe boi' does not match \"'float' object cannot be interpreted as an integer\".",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3472/204378389.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_on_numpy2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3472/2302202777.py\u001b[0m in \u001b[0;36mtest_on_numpy2\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mexpected_error_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"hehe boi\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpected_error_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\_pytest\\_code\\code.py\u001b[0m in \u001b[0;36mmatch\u001b[1;34m(self, regexp)\u001b[0m\n\u001b[0;32m    658\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mregexp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\" Did you mean to `re.escape()` the regex?\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 660\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    661\u001b[0m         \u001b[1;31m# Return True to allow for \"assert excinfo.match()\".\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Regex pattern 'hehe boi' does not match \"'float' object cannot be interpreted as an integer\"."
     ]
    }
   ],
   "source": [
    "test_on_numpy2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The well tested function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for these argument types\n",
    "- Bad arguments  -> raises an exception\n",
    "- Special arguments  -> boundary values, special logic\n",
    "- Normal arguments (at least 2 or 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_one_tab_with_missing_value():    # (1, 1) boundary value\n",
    "    actual = row_to_list(\"\\t4,567\\n\")\n",
    "    # Format the failure message\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal arguments example\n",
    "\n",
    "import pytest\n",
    "from preprocessing_helpers import row_to_list\n",
    "\n",
    "def test_on_normal_argument_1():\n",
    "    actual = row_to_list(\"123\\t4,567\\n\")\n",
    "    # Fill in with the expected return value for the argument \"123\\t4,567\\n\"\n",
    "    expected = [\"123\", \"4,567\"]\n",
    "    assert actual == expected, \"Expected: {0}, Actual: {1}\".format(expected, actual)\n",
    "    \n",
    "def test_on_normal_argument_2():\n",
    "    actual = row_to_list(\"1,059\\t186,606\\n\")\n",
    "    expected = [\"1,059\", \"186,606\"]\n",
    "    # Write the assert statement along with a failure message\n",
    "    assert actual == expected, \"Expected: {0}, Actual: {1}\".format(expected, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Driven Development (TDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write unit tests before implementation\n",
    "    - Unit tests cannot be deprioritized\n",
    "    - Time for writing unit tests factored in implementation time\n",
    "    - Requirements are clearer and implementation easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to organize a growing set of tests ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test class\n",
    "import pytest\n",
    "from data.preprocessing_helpers import row_to_list, convert_to_int\n",
    "\n",
    "class TestRowToList(object):\n",
    "    def test_on_no_tab_no_missing_value(self):\n",
    "        pass\n",
    "    \n",
    "    def test_on_two_tabs_no_missing_value(self):\n",
    "        pass\n",
    "\n",
    "class TestConvertToInt(object):\n",
    "    def test_with_no_comma(self):\n",
    "        pass\n",
    "    def test_with_one_comma(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final test directory structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![final test directory](tets_dir.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runing all tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd tests\n",
    "# pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- recurses into directory subtree of tests/\n",
    "    - filename starting with test_ -> test module\n",
    "        - classname starting with Test -> test class\n",
    "            - function names starting with test_ -> unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytest -x  -> stops after the first error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if I want to run specific tests\n",
    "\n",
    "# Node ID of a test class\n",
    "# <path to test module>::<test class name>\n",
    "\n",
    "# Node ID of an unit test\n",
    "# <path to test module>::<test class name>::<unit test name>\n",
    "\n",
    "#  ~~~~~~~~~~~~~ example ~~~~~~~~~~~~~~~~~~\n",
    "# ~$ pytest tests/data/test_preprocessing_helpers.py::TestRowToList::test_on_one_tab_with_missing_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytest -k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytest -k \"TestSplit and not test_on_one_row\"\n",
    "\n",
    "# TestSplit unique name, il reconnaitle nom complet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected failures and conditional skipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xfail: marking tests as expecting to fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "class TestTrainModel(object):\n",
    "    @pytest.mark.xfail(reason=\"Using TDD, train_model() is not implemented\")\n",
    "    def test_on_linear_data(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests that are expected to fail: skipif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error on python 2,7 or above\n",
    "\n",
    "import sys\n",
    "\n",
    "class TestConvertToInt(object):\n",
    "    @pytest.mark.skipif(sys.version_info > (2,7), reason=\"Works only on Python 2.7 or lower\")\n",
    "    def test_with_no_comma(self):\n",
    "        \"\"\"Only runs on python 2.7 or lower\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing reason for both skipped and xfail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytest -rsx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous integration and code coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For continuous integration, create a configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repository root\n",
    "#|-- src\n",
    "#|-- tests\n",
    "#|-- .travis.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contents of .travis.yml\n",
    "\n",
    "\"\"\"\n",
    "language: python\n",
    "python:\n",
    "    - \"3.6\"\n",
    "install:\n",
    "    - pip install -e .\n",
    "    - pip install pytest-cov codecov  # Install packages for code coverage report\n",
    "script:\n",
    "    - pytest tests         # Replace with line below if using code coverage\n",
    "    - pytest --cov=src tests  # Point to the source directory.\n",
    "after_success:\n",
    "    - codecov              # uploads report to codecov.io\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the file to github\n",
    "\n",
    "# git add .travis.yml\n",
    "# git push origin master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the travis CI app (free for public repos) from github\n",
    "# install codecov.io in github marketplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all this new knowledge, let's try to create a dummy project, with libraries and test units + continuous integration !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyon assertion: setup and teardown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def my_fixture():\n",
    "    # Do setup here\n",
    "    yield data\n",
    "    # Do teardown here\n",
    "\n",
    "def test_something(my_fixture):\n",
    "    data = my_fixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "import os\n",
    "\n",
    "@pytest.fixture\n",
    "def raw_and_clean_data_file():\n",
    "    # Setup\n",
    "    raw_data_file_path = \"raw.txt\"\n",
    "    clean_data_file_path = \"clean.txt\"\n",
    "    with open(raw_data_file_path, \"w\") as f:\n",
    "        f.write(\"1,801\\t201,411\\n\"\"1,767565,112\\n\"\"2,002\\t333,209\\n\")\n",
    "    yield raw_data_file_path, clean_data_file_path\n",
    "    # Teardown\n",
    "    os.remove(raw_data_file_path)\n",
    "    os.remove(clean_data_file_path)\n",
    "\n",
    "def test_on_raw_data(raw_and_clean_data_file):\n",
    "    raw_path, clean_path = raw_and_clean_data_file\n",
    "    preprocess(raw_path, clean_path)\n",
    "    with open(clean_data_file_path) as f:\n",
    "        lines = f.readlines()\n",
    "    first_line = lines[0]\n",
    "    assert first_line == \"1,801\\t201,411\\n\"\n",
    "    second_line = lines[1]\n",
    "    assert second_line == \"2,002\\t333,209\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tmpdir and fixture chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def raw_and_clean_data_file(tmpdir):\n",
    "    # Setup\n",
    "    raw_data_file_path = tmpdir.join(\"raw.txt\")\n",
    "    clean_data_file_path = tmpdir.join(\"clean.txt\")\n",
    "    with open(raw_data_file_path, \"w\") as f:\n",
    "        f.write(\"1,801\\t201,411\\n\"\"1,767565,112\\n\"\"2,002\\t333,209\\n\")\n",
    "    yield raw_data_file_path, clean_data_file_path\n",
    "    # No teardown code necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the correct argument to use the mocking fixture in this test\n",
    "def test_on_raw_data(self, raw_and_clean_data_file, mocker):\n",
    "    raw_path, clean_path = raw_and_clean_data_file\n",
    "    # Replace the dependency with the bug-free mock\n",
    "    convert_to_int_mock = mocker.patch(\"data.preprocessing_helpers.convert_to_int\",\n",
    "                                       side_effect=convert_to_int_bug_free)\n",
    "    preprocess(raw_path, clean_path)\n",
    "    # Check if preprocess() called the dependency correctly\n",
    "    assert convert_to_int_mock.call_args_list == [call(\"1,801\"), call(\"201,411\"), call(\"2,002\"), call(\"333,209\"), call(\"1990\"), call(\"782,911\"), call(\"1,285\"), call(\"389129\")]\n",
    "    with open(clean_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    first_line = lines[0]\n",
    "    assert first_line == \"1801\\\\t201411\\\\n\"\n",
    "    second_line = lines[1]\n",
    "    assert second_line == \"2002\\\\t333209\\\\n\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe06f96167c4fc64a78c238d993189072a4e72b444216e36203d6f96126eaf0a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
